#!/bin/bash
#SBATCH --job-name test1
#SBATCH --qos standard
#SBATCH --time 2-0:00:00
#SBATCH --account dp208
#SBATCH --nodes=1
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=8
#SBATCH --partition=gpu
#SBATCH --gres=gpu:4
#SBATCH --output=%x.%j.out
#SBATCH --error=%x.%j.err

module purge
module load cuda/11.4.1  openmpi/4.1.1-cuda11.4.1  ucx/1.12.0-cuda11.4.1

export LD_LIBRARY_PATH=/home/dp208/dp208/dc-forz1/grid/prefix/lib:/mnt/lustre/tursafs1/apps/basestack/cuda-11.4.1/ucx/1.12.0-cuda11.4.1/lib:/mnt/lustre/tursafs1/apps/basestack/cuda-11.4.1/openmpi/4.1.1-cuda11.4.1/lib:/mnt/lustre/tursafs1/apps/cuda/11.4.1/lib64/

export OMP_NUM_THREADS=4
export OMPI_MCA_b56tl=^uct,openib
export UCX_TLS=gdr_copy,rc,rc_x,sm,cuda_copy,cuda_ipc
export UCX_RNDV_SCHEME=put_zcopy
export UCX_RNDV_THRESH=16384
export UCX_IB_GPU_DIRECT_RDMA=yes
export UCX_MEMTYPE_CACHE=n


mpirun -np 1 -x LRANK=0 -x LD_LIBRARY_PATH  --bind-to none ./wrappersimple.sh ./Test_WilsonFlow --confstart 2551 --confend 2750 --confsave /scratch/s.2227764/WF_confs_vol12_final > ./hmc_${SLURM_JOB_ID}_vol12_b68-am08_2551-2750.out &

mpirun -np 1 -x LRANK=1 -x LD_LIBRARY_PATH  --bind-to none ./wrappersimple.sh ./Test_WilsonFlow --confstart 2751 --confend 2950 --confsave /scratch/s.2227764/WF_confs_vol12_final > ./hmc_${SLURM_JOB_ID}_vol12_b68-am08_2751-2950.out &

mpirun -np 1 -x LRANK=2 -x LD_LIBRARY_PATH  --bind-to none ./wrappersimple.sh ./Test_WilsonFlow --confstart 2951 --confend 3150 --confsave /scratch/s.2227764/WF_confs_vol12_final > ./hmc_${SLURM_JOB_ID}_vol12_b68-am08_2951-3150.out &

mpirun -np 1 -x LRANK=3 -x LD_LIBRARY_PATH  --bind-to none ./wrappersimple.sh ./Test_WilsonFlow --confstart 3151 --confend 3350 --confsave /scratch/s.2227764/WF_confs_vol12_final > ./hmc_${SLURM_JOB_ID}_vol12_b68-am08_3151-3350.out &

wait
